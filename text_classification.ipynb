{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import urllib.request\n",
    "from lxml import etree\n",
    " \n",
    "import numpy as np\n",
    " \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet=\"abcdefghijklmnopqrstuvwxyzäö-\"\n",
    "alphabet_set = set(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a list of Finnish words\n",
    "def load_finnish():\n",
    "    finnish_url=\"https://www.cs.helsinki.fi/u/jttoivon/dap/data/kotus-sanalista_v1/kotus-sanalista_v1.xml\"\n",
    "    filename=\"src/kotus-sanalista_v1.xml\"\n",
    "    load_from_net=False\n",
    "    if load_from_net:\n",
    "        with urllib.request.urlopen(finnish_url) as data:\n",
    "            lines=[]\n",
    "            for line in data:\n",
    "                lines.append(line.decode('utf-8'))\n",
    "        doc=\"\".join(lines)\n",
    "    else:\n",
    "        with open(filename, \"rb\") as data:\n",
    "            doc=data.read()\n",
    "    tree = etree.XML(doc)\n",
    "    s_elements = tree.xpath('/kotus-sanalista/st/s')\n",
    "    return list(map(lambda s: s.text, s_elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_english():\n",
    "    with open(\"src/words\", encoding=\"utf-8\") as data:\n",
    "        lines=map(lambda s: s.rstrip(), data.readlines())\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(a):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    count_vectorizer = CountVectorizer(analyzer=\"char\", vocabulary=alphabet)\n",
    "    count_vector = count_vectorizer.transform(a)\n",
    "    feature_matrix = count_vector.toarray()\n",
    "\n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_valid_chars(s):\n",
    "    for char in s:\n",
    "        if char not in alphabet_set:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_and_labels():\n",
    "    # processing finnish words\n",
    "    print(\"Processing finnish words\")\n",
    "    fi = load_finnish()\n",
    "    \n",
    "    #Convert the Finnish words to lowercase, and \n",
    "    fi2 = [x.lower() for x in fi]\n",
    "    # filter out words containing characters that don’t belong to the alphabet.\n",
    "    fi3 = [x for x in fi2 if alphabet_set.issuperset(x)]\n",
    "    fi_labels = [0 for x in fi3]\n",
    "    fi4 = np.array(fi3)\n",
    "    print(fi3)\n",
    "    print(type(fi3))\n",
    "    fif = get_features(fi4)\n",
    "    \n",
    "    print(\"Processing english words\")\n",
    "    en = load_english()\n",
    "    en = list(en)\n",
    "    en2 = [x for x in en if x[0].islower()]\n",
    "    #Convert the english words to lowercase, and \n",
    "    en3 = [x.lower() for x in en2]\n",
    "    # filter out words containing characters that don’t belong to the alphabet.\n",
    "    en4 = [x for x in en3 if alphabet_set.issuperset(x)]\n",
    "    en_labels = [0 for x in en4]\n",
    "    en5 = np.array(en4)\n",
    "    enf = get_features(en5)\n",
    "    \n",
    "    fien_labels = fi_labels+en_labels\n",
    "    fien_labels2 = np.array(fien_labels)\n",
    "    print(\"Both english and finnish words processed\")\n",
    "    print(type(fif))\n",
    "    print(type(enf))\n",
    " \n",
    "    # Stacking the two arrays along axis 0 \n",
    "    fifenf = np.stack((fif, enf), axis = 0)  \n",
    "    A = fif\n",
    "    B = enf\n",
    "    \n",
    "    C = np.vstack((A, B))\n",
    "    print(A.shape,B.shape,C.shape)\n",
    "    print(\"Exiting the function\")\n",
    "    return C, fien_labels2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_classification():\n",
    "    X, y = get_features_and_labels()\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=0)\n",
    "    #model = naive_bayes.GaussianNB()\n",
    "    model_selection.KFold(n_splits=5,shuffle=True,random_state=0)\n",
    " \n",
    "    model = MultinomialNB()\n",
    "    #model.fit(X_train, y_train)\n",
    "    #y_predicted = model.predict(X_test)\n",
    "    #print(metrics.accuracy_score(y_test, y_predicted))\n",
    " \n",
    "    scores = cross_val_score(model, X,y,cv=5)\n",
    "    #clf = svm.SVC(kernel='poly', C=1)\n",
    " \n",
    "    print(scores)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue dowloading the wordlist...\n",
    "\n",
    "# print(\"Accuracy scores are:\", word_classification())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
